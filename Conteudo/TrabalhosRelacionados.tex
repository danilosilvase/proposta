\chapter{Trabalhos Relacionados}

Nesta seção, identificamos e discutimos potenciais problemas no contexto da computação em nevoeiro em relação a gerenciamento de recursos e eficiência energética. Alguns deles podem ser a direção para trabalhos futuros. 

\section{Alocação de Recursos}

Kaur, Kuljeet, et al. \cite{kaur2017container}, apresentam CoESMS, uma arquitetura de software para seleção e escalonamento de tarefas na borda da rede utilizando container-as-a-service (CoaaS). Os autores propõem modelos baseados na teoria dos Jogos, a fim de minimizar a utilização total de energia dos servidores enquanto escalonam as tarefas em contêineres. A internet das coisas consiste em bilhões de dispositivos inteligentes interligados para atender demandas em áreas como saúde, transportes, agricultura, etc. Considerando o aumento na última década na adoção destes dispositivos e o volume de informações gerados de forma exponencial, faz-se necessário a adoção de métodos eficientes para processar e analisar estes dados \cite{evans2011internet}. Além disso o grande número de interconexões também estará associados ao tráfego de rede em tempo real para acessar diferentes serviços. Isso exigiria computação e processamento contínuos dos dados coletados de vários dispositivos com capacidades heterogêneas.

Métricas de desempenho, como por exemplo, poder computacional, entrega de serviços, desempenho, confiabilidade, escalabilidade e mobilidade apresentaram melhorias significativas neste contexto com avanços tecnológicos recentes [8]. Contudo, em contraste com este cenário, o tempo de resposta, custo por serviço, investimento em infra-estrutura e tempo de latência diminuíram gradualmente com o aumento dos avanços tecnológicos. Considerando estas questões, os autores adotam o conceito de Fog Computing \cite{computing2006architectural}, este conceito amplia a noção de computação em nuvem para um ambiente mais próximo ao cliente, onde os requisitos de computação, rede e armazenamento seriam atendidos na borda da rede.

Para suportar a transmissão de dados em tempo real de forma eficiente, é necessário agendar as várias tarefas de forma ideal para atender a vários parâmetros do SLA, como o tempo de resposta, a disponibilidade do serviço, a latência e os custos. Com este propósito, os autores desenvolveram uma função multi-objetivo para reduzir o consumo de energia e o makespan, ou  seja, o tempo total de processamento de todas as tarefas em todas as máquinas, considerando diferentes restrições, como memória, CPU e orçamento do usuário. 

A arquitetura proposta neste trabalho é composta por três entidades principais: consumidores de recursos, provedor de serviços públicos e fornecedores que são subdivididos em cinco camadas diferentes, apresentadas a seguir.

Resource Consumer Layer, esta camada compreende os consumidores de recursos da Fog, como clientes de TI e usuários finais, ou seja, esta camada é responsável apenas pela agregação e redirecionamento das solicitações de tarefas do usuário para o CoESMS.

Requirement Collector Layer, esta camada compreende dois módulos: o módulo de requisição de tarefa (TRM) e o módulo de solicitação de tarefa do usuário (UTRM). Usando esses dois módulos, as tarefas que seriam executadas na borda serão selecionadas, enquanto o resto das tarefas será encaminhado para o núcleo da rede.

Controller Layer, esta camada compreende dois módulos: o módulo de monitoramento de recursos (RMM) e o módulo de agendamento de contêineres (CSM). O RMM mantém continuamente o controle dos recipientes inativos que estão disponíveis para agendamento de tarefas. Com base nas informações recebidas deste módulo, o CSM escalona as tarefas nos contêineres de forma eficiente de energia usando técnicas baseadas na teoria dos jogos.

Broker Layer, Esta camada foi projetada para selecionar, classificar e reservar combinações de recursos na forma de contêineres atuando entre as camadas 3 e 5. Isto é alcançado com a ajuda do corretor que pertence à plataforma Fog Computing. A principal responsabilidade deste corretor é gerenciar os serviços do usuário, acompanhar as VMs e os contêineres disponíveis e delegar os pedidos aos contêineres das VMs.	

Computation Layer, esta camada compreende os nDCs (Nano Datacenters) que  executam as tarefas usando os contêineres. Ela também incorpora um controlador local a nível de nó que faz o controle de todos os contêineres no nível nDC e seu status atual. Dependendo do seu status atual, o controlador seleciona os contêineres que podem ser alocados para diferentes tarefas. Além disso, ele também executa migrações de contêiner entre as VMs. Essas migrações são realizadas para reduzir o consumo total de energia dos nDCs durante falhas inesperadas.


Na etapa de seleção e escalonamento de tarefas, foram utilizados contêineres em vez das máquinas virtuais convencionais para reduzir a sobrecarga e o tempo de resposta, bem como o consumo global de energia dos dispositivos da FOG, ou seja, nano data centers (nDCs). Em comparação com máquinas virtuais (VMs), os contêineres são instâncias de virtualização relativamente leves \cite{pahl2015containers,pahl2015containerization}. Os autores consideram como principais vantagens no uso de contêineres ao invés de máquinas virtuais características importantes que incluem leveza, desempenho, eficiência e ausência da necessidade de obtenção de instruções privilegiadas. Nesse contexto, além da arquitetura descrita, os autores apresentam uma técnica de migração em tempo real para minimizar o consumo de energia. A abordagem que desencadeia a migração dos contêineres é baseada no uso de energia dos servidores. Assim, a migração deverá iniciar sempre que a utilização do servidor exceda ou diminua considerando os limites máximos e inferiores pré definidos, respectivamente. 

A metodologia de avaliação deste trabalho foi proposta com base no impacto do CoESMS na utilização global de energia utilizando Fog Computing, ao mesmo tempo que garante níveis aceitáveis de SLA. O ambiente é implementado usando uma versão estendida do popular simulador de nuvem CloudSim, ContainerCloudSim \cite{piraghaj2017containercloudsim}. A ferramenta proporciona um ambiente contêineres em relação à computação em nuvem para modelagem e simulação. Nesta etapa é feita a comparação do esquema proposto com um esquema que não implementa o CoESMS. A Figura 2 apresenta as características do ambiente de simulação.



Os testes experimentais da simulação foram repetidos 25 vezes em um período de simulação de 24 horas. Para avaliar os dois esquemas com base na configuração de simulação acima, foram utilizadas cargas de trabalho com base no PlanetLab \cite{chun2003planetlab}. O desempenho dos dois esquemas foi comparado utilizando as seguintes quatro métricas de desempenho: número total de migrações de contêineres, análise de sobrecarga de contêineres, número médio de violações de SLA e total de energia consumida de (kWh), as figuras 3 e 4 apresentam respectivamente os resultados apresentados.



De acordo com os autores, os contêineres são entidades virtuais mais leves do que VMs e suportam migrações internas e externas contínuas, sem impor nenhuma sobrecarga significativa na CPU e na utilização da memória do subjacente e na largura de banda da rede. Não obstante, os resultados apresentados demonstram que o esquema proposto reduz o consumo de energia e o número médio de violações de SLA em 21,75\% e 11,82\%, respectivamente. Os resultados podem ser atribuídos ao fato de que o esquema proposto mapeia as tarefas para os contêineres com base na abordagem multi-objetivo da teoria dos Jogos, que formula o jogo entre os contêineres e seleciona a tarefa considerada a melhor para a execução. Quando a melhoria na eficiência energética pode ser atribuída ao escalonamento de tarefas com eficiência energética em contêineres e suas migrações.

\section{Qualidade de Serviço}

Em \cite{suto2015energy}, Suto et al. propuseram um esquema de operação de um sistema de computação sem fio (WCS) baseado em Fog Computing eficiente em termos de energia e atraso de pacotes. Neste trabalho os autores consideram que existe uma relação de compensação entre o consumo de energia e o atraso para a coleta de dados, assim, o esquema proposto controla o tempo de desligamento das antenas e a conectividade de rede para reduzir o consumo de energia do sistema enquanto satisfazem um atraso de pacotes aceitável, o qual é decidido com base na exigência de aplicações IoT industriais. A adoção da Internet das coisas em uma uma fábrica inteligente introduz uma nova revolução na era industrial, onde serviços como um sistema de computação coletam vários tipos de dados de máquinas e sensores podem extrair uma grande quantidade de dados coletados obtendo informações valiosas em tempo real para a operação da fábrica \cite{salvadori2009monitoring,ovsthus2014industrial}. Assim, torna-se possível otimizar a operação da fábrica sem a intervenção direta de recursos humanos. 

Neste cenário, os autores consideraram que, em comparação com os serviços providos diretamente pela nuvem, o uso de Fog computing - onde os nós de computação estão localizados fisicamente mais pŕoximos dos sensores para se comunicar diretamente com os nós do gateway - a rede baseada em Fog pode reduzir drasticamente a latência do feedback. Não obstante, a fim de obter capacidade suficiente para fornecer esse serviço, um nó de computação de nevoeiro deve satisfazer requisitos como, alto desempenho de processamento para suportar a grande exploração de dados em tempo real, coleta de dados simultânea de muitos sensores, alta disponibilidade de serviços e baixo consumo de energia do sistema para uma operação de fábrica de baixo custo.

Com o objetivo de atender a estes critérios, este trabalho se concentra em um Sistema de Computação Sem Fio  (WCS) \cite{shin2013feasibility,suto2015failure}, uma vez que o WCS efetivamente acomoda muitos servidores, ele pode alcançar uma alta capacidade de processamento, mesmo que o espaço seja limitado. Além disso cada nó foi equipado com antenas de 60 GHz onde a alta taxa de transmissão de dados - entre  4 e 15 Gbps permite coletar dados de muitos sensores simultaneamente.

Uma vez que o serviço IoT é sensível ao atraso, faz-se necessário satisfazer um atraso aceitável para a coleta de dados. Além disso, em um ambiente industrial é uma tarefa primordial reduzir o consumo de energia para alcançar uma fábrica eficiente de energia. Assim, para abordar esta questão de pesquisa, os autores construíram um modelo matemático para avaliar o consumo de energia do sistema e o atraso na coleta de dados, e assim mostrar uma relação de troca entre esses valores. O modelo apresentado considera que para executar o controle de feedback em tempo real, o sistema de computação coleta vários tipos de dados de sensores que são implantados em toda a fábrica, que denotam o conjunto de tipos de dados. Além disso, uma vez que o sistema de computação possui conhecimento sobre dinâmicas de dados (ou seja, tempo de chegada de dados) e o atraso interno aceitável de cada dado é previamente definido, ele conhece o atraso interno aceitável do tipo de dados.

Além disso, com base no trade-off, os autores propuseram um sistema de computação sem fio eficiente em energia e atraso (E2DA-WCS). Na arquitetura proposta, a proporção de servidores em estado de suspensão e modo dos servidores em estado ativo em cada intervalo de tempo são controlados dinamicamente para minimizar o consumo de energia do sistema enquanto satisfazem um atraso interno aceitável.

A avaliação de desempenho do esquema proposto considerando eficiência energética e atraso foi previsto usando simulações computacionais. O cenário de avaliação considera um ambiente com constituído por 100 servidores executando a simulação durante 100 segundos. Além disso, a perda do caminho é calculada usando as equações descritas em \cite{suto2015failure}. Para verificar a eficácia a proposta apresentada a mesma foi comparada os esquemas convencionais. Como um dos esquemas convencionais, os autores utilizaram um esquema que controla dinamicamente o tempo de desligamento, mas usa um grau constante, conhecido como sono dinâmico com grau constante (DSCD). O outro esquema, referido como Sono constante com grau constante (CSCD), usa consistentemente um número constante de servidores no estado de suspensão e o grau de servidores no estado ativo. As figuras I e II apresentam os resultados obtidos.

A figura I apresenta a quantidade de consumo de energia do sistema por um período de 100 segundos. O consumo de energia do sistema é calculada como a soma da potência consumida no instante t de 0 a 100 segundos. Conforme mostrado, no caso do cenário 1, proposta atinge aproximadamente 13 e 10 por cento de redução no consumo de energia do sistema em comparação com DSCD e CSCD, respectivamente. Além disso, a figura II demonstra o índice de satisfação de atraso nos diferentes cenários. A partir deste resultado, os autores concluem que a metodologia apresentada pode atingir a proporção máxima no ambiente de simulação. Por outro lado, o DSCD atinge cerca de 60 por cento do índice de satisfação no cenário 1 e a proporção diminui ainda mais no cenário 2. A partir desse fenômeno, podemos notar que o grau constante não pode satisfazer o atraso aceitável e o controle conjunto do tempo de desligamento e o estado dos servidores tem um grande impacto na relação de satisfação.
Neste trabalho, os autores investigaram um novo sistema de computação para fábricas inteligentes com IoT e Fog Computing. Além disso, criaram um modelo matemático para avaliar o desempenho do sistema proposto com base na complexa teoria da rede. Este modelo mostrou a existência de uma relação de trade-off entre consumo de energia e atraso necessário para a coleta de dados. Consequentemente, foi proposto um esquema de operação eficiente em termo de consumo de energia 
e atraso de pacotes.

\section{Arquiterura verde}

 Sakar et al. \cite{sarkar2016theoretical}, apresentam um modelo teórico para arquitetura de computação em nevoeiro e compara seu desempenho com o modelo tradicional de computação em nuvem. Este trabalho foi motivado pela necessidade de disponibilizar serviços de baixa latência em tempo real simultaneamente para bilhões de dispositivos IoT na borda da rede diante da arquitetura tradicional da computação em nuvem. É importante observar que estes dois paradigmas não substituem um ao outro, e sim se complementam.
    Nesta direção, os autores propuseram uma formulação matemática para o paradigma computacional “Fog Computing”, definindo seus componentes individuais e apresentando um estudo comparativo com a computação em nuvem em termos de latência de serviço e consumo de energia. O modelo apresentado
As métricas definidas neste trabalho para análise de desempenho da computação em nevoeiro em comparação com o modelo de computação em nuvem tradicional são a latência de serviço e consumo de energia. A latência do serviço é o tempo de resposta para uma solicitação enviada por uma instância do aplicativo que está sendo executada dentro de um dispositivo IoT, esta métrica é ‘calculada como a soma da latência de transmissão e a latência de processamento para a solicitação. Por outro lado, o consumo de energia é medido como a energia gasta durante à transmissão de bytes da unidade de dados do nível inferior para a camada intermediária, e do nível intermediário para a camada superior, ou seja, todo o caminho que os dados percorrem entres os dispositivos IoT, passando pela camada de computação em nevoeiro até a nuvem.
O cenário do experimento considera a velocidade de processamento dos dispositivos no nível de computação em nevoeiro equivalentes ao processador ARM Cortex A5 e os centros de dados da nuvem são tomados como processadores Intel Core i7 4770k respectivamente.
Segundo os autores, os resultados mostram que, para um cenário em que 25\% dos aplicativos IoT exigem serviços em tempo real e de baixa latência, o gasto médio de energia na computação em nevoeiro é 40,48\% menor do que o modelo convencional de computação em nuvem. A partir desta análise de desempenho, os autores discutem a aplicabilidade da vida real do paradigma da computação em nevoeiro, juntamente com a estrutura tradicional de computação em nuvem, e algumas de suas implementações práticas. Observou-se que, para um sistema com grande número de aplicações IoT em tempo real e de baixa latência, a latência de serviço associada à computação em nevoeiro foi significativamente menor que a da computação em nuvem.



\section{Otimização de Microprocessadores}

Em \cite{nassiffe2016optimising}, Nassiffe et al. apresentam um mecanismo para selecionar dinamicamente a freqüência da CPU para executar tarefas considerando restrições de escalabilidade e disponibilidade de energia. O mecanismo proposto leva em consideração um cenário onde a carga computacional das tarefas não é constante, mas varia de forma dinâmica ao longo do tempo.  Segundo os autores, é crescente o número de sistemas embarcados que dependem de baterias para operar. Tais sistemas são encontrados em muitas aplicações, entre elas, entretenimento, saúde, agricultura e etc. Além disso, estes devem ser capaz de tomar decisões em ambientes desestruturados, ou seja, onde as condições ambientais não podem ser previamente determinadas ou que realizem missões que possam pôr em perigo a vida humana ao mesmo tempo em que o sistema conseve energia e mantenha níveis aceitáveis de QoS. \cite{nassiffe2016optimising,raibert2008bigdog}. 
Por outro lado, a escala dinâmica de tensão e frequência (DVFS) é uma técnica comumente utilizada para obter economias de energia em sistemas em tempo real. Diversos trabalhos utilizam esta técnica na tentativa de economizar energia em sistemas embarcados \cite{saha2012experimental,snowdon2005power,zhu2004effects,aydin2006system}, contudo, os autores alegam que nem sempre questões como, outros dispositivos além da CPU e QoS nem sempre são considerados nos trabalhos. Portanto, neste trabalho, a metodologia apresentada parte da necessidade de ajustar a freqüência de acordo com o tempo necessário para realizar cada tarefa, a carga da CPU e a energia disponível considerando que que o sistema ofereça serviços com a melhor qualidade possível.

O mecanismo desenvolvido baseia-se em métodos de ponto interior utilizados como um meio de otimizar a QoS do sistema sujeito a restrições de energia e agendamento. O problema é formulado em programação matemática e resolvido com um algoritmo de otimização convexa onde supõe-se que a frequência da CPU pode ser selecionada continuamente dentro de um determinado intervalo. Tais métodos são conhecidos por serem computacionalmente eficientes. Como o problema de reconfiguração definido é convexo, ele pode ser resolvido usando o método do ponto interior que garante a convergência para o ótimo global \cite{boyd2004convex}. Este método pode ser visto como uma estrutura para problemas convexos, pelo que o método de Newton é aplicado a uma seqüência de aproximações sem restrições. 

A eficácia do mecanismo de reconfiguração proposto foi apresentado através de uma análise numérica do algoritmo. Assim, o desempenho da abordagem foi avaliado verificando o número de iterações necessárias para resolver problemas divididos em fases. Para isso, foram criados 1000 conjuntos de tarefas sintéticas com 10, 30 e 50 tarefas cada um usando o algoritmo UUniFast \cite{bini2005measuring}. O diagrama de caixa  apresenta os resultados obtidos no experimento.

A análise apresentado indica que quando o número de tarefas cresce, o algoritmo torna-se mais sensível aos parâmetros que definem a instância do problema, onde o número de iterações os tende a variar mais com 50 tarefas do que 10 ou 30. Por outro lado, o número de iterações para resolver o problema não aumentou na mesma taxa do número de tarefas considerando que, quando o conjunto de tarefas aumenta de 10 a 50 tarefas, 500 por cento , o número de iterações a resolver aumentou 178 por cento. Além disso, os autores consideram que uma solução viável pode ser conseguida resolvendo apenas uma das fases.


Por outro lado, Zahaf e Houssam-Eddine et al. \cite{zahaf2017energy} propõem uma heurística para paralelização e alocação de threads em plataformas multicore heterogêneas ajustando dinamicamente os níveis de energia dos núcleos com o objetivo de reduzir o consumo total de energia sem comprometer o sistema. Neste trabalho os autores apresentam um modelo de consumo de energia e desempenho para execução de tarefas paralelas em tempo real para a arquitetura ARM big.LITTLE. O consumo de energia é considerado um fator de extrema importância em sistemas computacionais de tempo real, principalmente quando operados com a energia de bateria. Este cenário se agrava quando os sistemas distribuídos em tempo real integrados devem suportar aplicações cada vez mais complexas coletando uma grande quantidade de dados de sensores. Um cenário típico para este tipo de aplicação é a Fog Computing, ou computação em nevoeiro, onde uma certa quantidade de dados coletada do meio ambiente, precisa ser pré-processada em tempo real antes de ser enviada para o servidor central para armazenamento e realizar processamento final. Segundo \cite{zahaf2017energy}, muitas dessas aplicações podem ser facilmente paralelizadas distribuindo dados através dos elementos de computação paralela. Por outro lado, a tecnologia de multicore heterogênea pode ajudar a alcançar a pontualidade e o baixo consumo de energia, mesmo quando a carga computacional não é muito alta, pois neste cenário, os processadores são mais eficientes que uma plataforma equivalente de um único núcleo \cite{wolf2012computers}. A ideia é operar o sistema em uma "freqüência mais baixa", definir alguns núcleos em um estado de energia profunda e, ao mesmo tempo, reduzir o tempo de resposta da tarefa ao decompor uma tarefa esporádica em threads paralelos para serem divididos em todos os núcleos ativos.
Modelos de tarefas paralelas de tempo real são classificadas de acordo com o nível de paralelismo (rígido, moldável e maleável), de acordo com, \cite{drozdowski2004scheduling,dutot2004scheduling} a maioria das aplicações paralelas no mundo real são moldáveis. Considerando estas questões, os autores se concentraram em tarefas moldáveis em tempo real. A fim de obter uma decomposição ideal em relação ao consumo de energia para um conjunto de tarefas na plataforma multi-core heterogênea, é necessário definir a freqüência de operação dos núcleos, decompor cada tarefa em um conjunto de threads paralelos e realizar uma análise de planejamento e alocar os segmentos para os núcleos para garantir que cada segmento seja concluído antes do prazo.



Por outro lado, os autores utilizam um modelo, onde definem o problema de otimização como um problema de Programação Não-linear Inteira (INLP). O problema da alocação de tarefas sem decomposição já é um problema NP completo, adicionando a decomposição e a seleção de freqüência, o problema torna-se mais difícil e não linear. Dadas essas definições, os autores apresentam uma formulação do problema como problema INLP. A função objetivo apresentada neste trabalho considera a soma de dois termos. O primeiro termo expressa a energia que depende da execução da thread enquanto o segundo termo expressa o consumo de energia comum que é gerado por pelo menos um segmento quando alocado. 
Além disso, os autores propõem uma heurística de escalonamento baseada em um algoritmo guloso para selecionar as freqüências e os pontos de corte e para alocar as threads nos diferentes núcleos disponíveis, de modo a minimizar o consumo total de energia, garantindo que todos os prazos sejam respeitados.
Para avaliar a abordagem, apresentam uma ampla gama de experiências sintéticas que demonstram o desempenho da abordagem proposta. A heurística foi aplicada em um conjuntos de tarefas sintéticas geradas aleatoriamente testados em dois cenários com base no método UUniFast \cite{davis2009priority}. A plataforma de experimento adotada consiste em placas ODROID XU32 compostas por um Samsung Exynos 5422, um GPU Mali, memória RAM e periféricos de E/S. Em cada experimento, foram alocados os segmentos de avaliação comparativo nos núcleos operando em uma freqüência fixa. Em cada cenário variou-se, o número de threads, o núcleo em que o segmento é alocado e a freqüência de operação desse núcleo. A figura 7 apresenta o consumo de energia para pequenos e grandes núcleos.


%\documentclass[twocolumn]{article}



Os resultados apresentados na figura I, traçam o consumo de energia de núcleos pequenos e grandes em função da utilização total. Para pequenos núcleos, o CPM consome mais energia do que as heurísticas bin-packing, ou ou problema do empacotamento. No entanto, esta energia é recuperada em núcleos grandes, onde o CPM consome menos energia que a heurísticas BF. Não obstante, os autores destacam como trabalhos futuros a implementação de técnicas de DVFS para tirar proveito desta característica e expandir o modelo de tarefa di-gráfico proposto por Stigge et al. em \cite{stigge2011digraph}, que é projetado para expressar esse tipo de tarefas dinâmicas.


\section{Considerações Finais do capítulo}

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
\begin{table*}
\centering
\caption{Visão geral de alguns aspectos da revisão bibliográfica}
\label{my-label}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}llclll@{}}
\toprule
Ano (Autor)           & Metodologia                                    & \multicolumn{1}{l}{Considera o nível de bateria?} & Técnica / Algoritmo    & Métricas                                 & Aplicação                               \\ \midrule
2015 (Suto et al.)    & Desligamento das antenas e comunicação         & Não                                               & Complex network theory & SLA e consumo de energia                 & Simulação (plataforma não especificada) \\
2016 (Nassife et al.) & Variação de fequência de CPU (DVFS)            & Sim                                               & UUniFast               & Consumo de energia                       & Simulação (Análise numérica)            \\
2016 (Sackar et al.)  & Modelo matemático                              & Não                                               & Não se aplica          & Latência de serviço e consumo de energia & Simulação (plataforma não especificada) \\
2017 (KAur et al.)    & Seleção e migração de tarefas                  & Não                                               & Teoria dos Jogos       & SLA e consumo de energia                 & Simulação (ContainerCloudSim)           \\
2017 (Zahaf et al.)   & Paralelização de tarefas e alocação de threads & Sim                                               & Bin packing            & Consumo de energia                       & Prototipagem (ARM big.LITTLE)           \\ \bottomrule
\end{tabular}%
}
\end{table*}
%\FloatBarrier

Como a computação de nevoeiro é um paradigma relativamente recente, a pesquisa ainda está em seu estágio inicial. O trabalho em curso elenca desafios que podem aprimorar as capacidades de gerenciamento de recursos e eficiência energética no contexto do IoT. Nesta direção, os trabalhos discutem a definição da arquitetura \cite{sarkar2016theoretical} e a avaliação da sua adequação no contexto de IoT \cite{gupta2016ifogsim}. Outros aspectos incluem alocação de recursos \cite{kaur2017container}, qualidade de serviço \cite{suto2015energy}, análise de dados em tempo real e consciente do contexto [9], bem como um modelo de otimização de microprocessadores \cite{nassiffe2016optimising} e escalonamento de tarefas para obter desempenho mantendo a eficiência energética \cite{zahaf2017energy}.

Em \cite{sarkar2016theoretical} observou-se que a latência de serviço associada à computação em nevoeiro foi significativamente menor que a da computação em nuvem. Não obstante, o trabalho não discutiu questões como gerenciamento de recursos e métodos de virtualização. 
Kaur et al. \cite{kaur2017container} propuseram uma arquitetura que utiliza métodos de seleção e escalonamento de tarefas para reduzir o consumo de energia ao mesmo tempo em que mantém requisitos de SLA. A utilização de contêiners mostrou-se promissora, contudo, o trabalho limitou-se a implementação em ambientes de simulação.  O trabalho apresentado por \cite{suto2015energy},  propõe um esquema que controla o tempo de desligamento das antenas e a conectividade de rede para reduzir o consumo de energia do sistema enquanto satisfazem níveis aceitáveis de SLA. 
Por outro lado, o trabalho proposto do Nassife \cite{nassiffe2016optimising}, apresenta um mecanismo utilizando métodos de ponto interno como meio para otimizar a QoS de sistemas de tempo real sujeitos a restrições de energia e escalonamento. Seu método mostrou-se promissor, contudo, a pesquisa não contemplou tarefas com mais de um modo de operação e valores discretos para a variação de frequência da CPU. A metodologia apresentada em \cite{zahaf2017energy} paraleliza e aloca threads em plataformas multicore heterogêneas, sua abordagem é eficaz quando comparada aos algoritmos clássicos de heurística bin-packing. Todavia, mecanismos para economia de energia que consideram o tempo ocioso do processador não foram apresentadas. 
    
Os dados apresentados da tabel I mostram que, embora muita pesquisa tenha se concentrado nos desafios relacionados à natureza e comunicação da IoT, os desafios da em relação aos microprocessadores e otimização dos dispositivos receberam muito menos atenção. Além disso,  embora a maior parte dos trabalhos apresentados discutam questões de consumo de energia, nem todos consideraram os víveis de bateria dos dispositivos. 